I have made a project "Book recommendation System" . Now i need to write a report on this.
Now write the following sections of the report in full detail:

1. section_1 name
2. section_2 name
.
.
.
etc

I am providng the the project code below for your refernce:


# Importing the required libraries
import numpy as np  # Numerical computing library
import pandas as pd  # Data manipulation library
pd.set_option("display.max_colwidth", 1000)  # Setting maximum column width for DataFrame display

# Data visualization libraries
import seaborn as sns  # Statistical data visualization
sns.set_style('white')
import matplotlib.pyplot as plt  # Plotting library

# Word cloud creation
from PIL import Image  # Python Imaging Library
from wordcloud import WordCloud  # Word cloud generator
from wordcloud import WordCloud, STOPWORDS  # Word cloud and stop words
plt.rcParams["figure.figsize"] = (8, 8)  # Setting default figure size
from IPython.display import Image, HTML  # Displaying images in IPython

# Suppressing warnings
import warnings
warnings.simplefilter('ignore')

# Other libraries
import os  # Operating system interface
import nltk  # Natural Language Toolkit
import sklearn  # Machine learning library


# Reading datasets into DataFrames
books_df = pd.read_csv('dataset/Books.csv')  # Reading books dataset into a DataFrame
users_df = pd.read_csv('dataset/Users.csv')  # Reading users dataset into a DataFrame
rating_df = pd.read_csv('dataset/Ratings.csv')  # Reading ratings dataset into a DataFrame

users_df.head(3)

books_df.head(3)

rating_df.head(3)


# Authored the most number of books
popular_authors = books_df.groupby('Book-Author')['Book-Title'].count().nlargest(10)

# Best selling authors
best_selling_authors = df.groupby('Book-Author')['User-ID'].nunique().nlargest(10)

# Creating pie charts
plt.figure(figsize=(16, 8))

# Pie chart for top ten writers in terms of number of books published
plt.subplot(1, 2, 1)
plt.pie(popular_authors, labels=popular_authors.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette('Paired'))
plt.title('Top Ten Writers in Terms of Number of Books Published')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle

# Pie chart for best selling authors
plt.subplot(1, 2, 2)
plt.pie(best_selling_authors, labels=best_selling_authors.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette('Paired'))
plt.title('Best Selling Authors')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle

plt.tight_layout()
plt.show()

# Companies with the most number of books published
popular_publishers = books_df.groupby('Publisher')['Book-Title'].count().nlargest(10)

# Creating a point plot
plt.figure(figsize=(10, 8))
sns.pointplot(x=popular_publishers.values, y=popular_publishers.index, color='blue')
plt.xlabel('Number of Books Published')
plt.ylabel('Publisher')
plt.title('Top Ten Publishers in Terms of Number of Books Published')
plt.show()

df.groupby('Book-Title')['User-ID'].count().nlargest(10)

# Top selling books
most_purchased_books = df.groupby('Book-Title')['User-ID'].nunique().nlargest(10)

# Creating a pie chart
plt.figure(figsize=(8, 8))
plt.pie(most_purchased_books, labels=most_purchased_books.index, autopct='%1.1f%%', startangle=140, colors=sns.color_palette('Paired'))
plt.title('Top Selling Books')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle
plt.show()

# Top-rated books
top_rated_books = df.groupby('Book-Title')['Book-Rating'].sum().nlargest(10)

# Creating a vertical bar plot
plt.figure(figsize=(10, 8))
sns.barplot(x=top_rated_books.index, y=top_rated_books.values, palette='Paired')
plt.xlabel('Book Title')
plt.ylabel('Total Rating')
plt.title('Top Rated Books')
plt.xticks(rotation=90)  # Rotate x-axis labels for better readability
plt.show()

fig = plt.figure(figsize=(20, 14))
i = 1
for group in ['Children', 'Teens', 'Youth', 'Middle aged adults', 'Elderly']:
    age_df = df.loc[df['Age_group'] == group].groupby(['Book-Title']).agg(No_of_users=('User-ID', 'nunique'),
                                                                         total_rating=('Book-Rating', 'sum')).reset_index()
    if not age_df.empty:
        plt.subplot(5, 2, i)
        age_df.sort_values(by='No_of_users', ascending=False, inplace=True)
        sns.barplot(x='No_of_users', y='Book-Title', palette='Paired', data=age_df.head(5))
        plt.title('Top 5 Popular books among {}'.format(group), size=16)
        i += 1
        plt.subplot(5, 2, i)
        age_df.sort_values(by='total_rating', ascending=False, inplace=True)
        sns.barplot(x='total_rating', y='Book-Title', palette='Set2', data=age_df.head(5))
        plt.title('Top rated books by {}'.format(group), size=16)
        i += 1
plt.tight_layout()
plt.show()

from IPython.display import Image, HTML   #Displaying images


#finding the average rating and number of votes received by books
df_relevant_data = df.groupby(['Book-Title','Book-Author'],as_index=False).agg(avg_rating=('Book-Rating','mean'),ratings_count=('Book-Rating','count'))
v=df_relevant_data['ratings_count']
R=df_relevant_data['avg_rating']
C=df_relevant_data['avg_rating'].mean()
m=int(df_relevant_data['ratings_count'].quantile(0.90))#minimum number of votes to be listed
print(f'The average rating of all the books is {C} and the minimum number of votes required by the books to be listed is {m}  ')

#Calculating weighted average rating of the books
df_relevant_data['weighted_average']=round(((R*v)+ (C*m))/(v+m),2)

df_relevant_data.sort_values(by='weighted_average',ascending=False).head(10)

"""This is the list of most favored books based on the weighted rating scores. The book 'Harry Potter and the Chamber of Secrets Postcard Book' seems to have top this chart.

## **Author based recommender system**
"""

def author_based(book_title,number,df_relevant_data=df_relevant_data):
  '''
  To recommend books from the same author as the book entered by the user
  '''
  author=df_relevant_data.loc[df_relevant_data['Book-Title']==book_title]['Book-Author'].unique()[0]
  author_df=df_relevant_data.loc[(df_relevant_data['Book-Author']==author)].sort_values(by='weighted_average',ascending=False)
  print(f'The author of the book {book_title} is {author}\n')
  print(f'Here are the top {number} books from the same author\n')
  top_rec=author_df.loc[(author_df['Book-Title']!=book_title),['Book-Title','weighted_average']].head(number)
  return(top_rec)

#get book name and number of books to recommend
book_title = 'Harry Potter and the Chamber of Secrets (Book 2)'
number =5
author_based(book_title,number)
# top_recommendations from the same author

"""# **Collaborative filtering**

### Collaborative filtering methods construct a model by analyzing historical user interactions, such as items bought or movies watched and rated. This model incorporates preferences expressed by both present and past users. Subsequently, the model is employed to forecast ratings for items or suggest items that align with the user's potential interests.
---
"""

## **Model Based Approach**


---


## 1. **Singular Value Decomposition**

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from scipy.sparse.linalg import svds
import random
import math

"""###**Filtering the number of books and users**"""

#Filtering books with more than 5 reviews

print('The number of books that are explicitely rated are',explicit_rating['ISBN'].nunique())
ratings_count_df=explicit_rating.groupby("ISBN")['User-ID'].count().to_frame('No-of-rated-users').reset_index()
selected_books =list(ratings_count_df.loc[ratings_count_df['No-of-rated-users']>5,'ISBN'].unique())
print('Number of  books rated by atleast 5 users:',len(selected_books))
filter_df=explicit_rating.loc[explicit_rating['ISBN'].isin(selected_books)]

#keeping books with selected users
print('The number of users who have explicitely rated books are',explicit_rating['User-ID'].nunique())

#keeps Users who have rated more than five books
books_count_df=filter_df.groupby("User-ID")['ISBN'].count().to_frame('No-of-books-rated').reset_index()
selected_users = list(books_count_df.loc[books_count_df['No-of-books-rated']>5,'User-ID'].unique())
print('Number of  users who have rated atleast 5 books are :',len(selected_users))

#dataframe with filtered number of interactions
filter_df=filter_df.loc[filter_df['User-ID'].isin(selected_users)]
print('The shape of data fame with filtered number of interactions : ',filter_df.shape)

complete_df = filter_df.copy()

complete_df['Book-Rating'].describe()

def smooth_user_preference(x):
    '''Function to smooth column'''
    return math.log(1+x, 2)
#applying function
complete_df['Book-Rating']= complete_df['Book-Rating'].apply(smooth_user_preference)
complete_df.head()

train_df, test_df = train_test_split(complete_df,
                                   stratify=complete_df['User-ID'],
                                   test_size=0.20,
                                   random_state=0)

print('# interactions on Train set: %d' % len(train_df))
print('# interactions on Test set: %d' % len(test_df))

#displaying the first 5 rows of test set
test_df.head()

#Creating a sparse pivot table with users in rows and ISBN number of books in columns
users_books_pivot_matrix_df = train_df.pivot(index='User-ID',
                                                          columns='ISBN',
                                                          values='Book-Rating').fillna(0)

users_books_pivot_matrix_df.head()

#Creating a matrix with the values of users_books_pivot_matrix_df
original_ratings_matrix = users_books_pivot_matrix_df.values
original_ratings_matrix[:10]

#Storing the User-IDs in a list
user_ids = list(users_books_pivot_matrix_df.index)
user_ids[:10]

# The number of factors to factor the user-item matrix.
NUMBER_OF_FACTORS_MF = 20

#Performs matrix factorization of the original user item matrix
U, sigma, Vt = svds(original_ratings_matrix, k = NUMBER_OF_FACTORS_MF)

#converting sigma to a diagonal matrix
sigma = np.diag(sigma)

""" After the factorization, we try to to reconstruct the original matrix by multiplying its factors. The resulting matrix is not sparse any more. It has generated rating predictions for books with which users have not yet interacted (and therefore not rated), which we will use to recommend relevant books to the user."""

#Rating matric reconstructed using the matrices obtained after factorizing
predicted_ratings_matrix = np.dot(np.dot(U, sigma), Vt)
predicted_ratings_matrix

#Converting the reconstructed matrix back to a Pandas dataframe
predicted_ratings_df = pd.DataFrame(predicted_ratings_matrix , columns = users_books_pivot_matrix_df.columns, index=user_ids).transpose()
predicted_ratings_df.head()

class CFRecommender:
    #Storing model name
    MODEL_NAME = 'Collaborative Filtering'

    def __init__(self, cf_predictions_df, items_df=None):
        #Creating attributes
        self.cf_predictions_df = cf_predictions_df
        self.items_df = items_df

    def get_model_name(self):
        '''This will return model name'''
        return self.MODEL_NAME

    def recommend_items(self, user_id, items_to_ignore=[], topn=10, verbose=False):
        # Get and sort the user's predictions
        sorted_user_predictions = self.cf_predictions_df[user_id].sort_values(ascending=False).reset_index().rename(columns={user_id: 'Book-Rating'})

        # Recommend the highest predicted rating content that the user hasn't seen yet.
        recommendations_df = sorted_user_predictions[~sorted_user_predictions['ISBN'].isin(items_to_ignore)].sort_values('Book-Rating', ascending = False).head(topn)

        if verbose:
            #runs only if verbose=True
            if self.items_df is None:
                raise Exception('"items_df" is required in verbose mode')
            #Merging
            recommendations_df = recommendations_df.merge(self.items_df, how = 'left',
                                                          left_on = 'ISBN',
                                                          right_on = 'ISBN')[["ISBN",'Book-Title',	'Book-Author', 'Year-Of-Publication',	'Publisher']]

        return recommendations_df

#Creating object of the class
cf_recommender_model = CFRecommender(predicted_ratings_df, books_df)

def get_items_interacted(person_id, interactions_df):
    '''
    This function will take user id as input and return interacted items
    '''
    interacted_items = interactions_df.loc[person_id]['ISBN']
    #Repetation is avoided by taking set
    return set(interacted_items if type(interacted_items) == pd.Series else [interacted_items])

#Indexing by personId to speed up the searches during evaluation
full_indexed_df =complete_df.set_index('User-ID')
train_indexed_df = train_df.set_index('User-ID')
test_indexed_df = test_df.set_index('User-ID')

#Recommendation for a single user
cf_recommender_model.recommend_items(user_ids[3],items_to_ignore= get_items_interacted(user_ids[3],train_indexed_df),verbose=True)

"""##**Model Evaluation**"""

# Function for getting the set of books which a user has not interacted with
def get_not_interacted_items_sample(person_id, sample_size, seed=42):
    #Storing interacted items
    interacted_items = get_items_interacted(person_id, full_indexed_df)
    #Getting set of all items
    all_items=set(full_indexed_df["ISBN"])
    #Obtaining non interacted items
    non_interacted_items = all_items - interacted_items

    random.seed(seed)
    #Selecting random sample of given sample_size
    #non_interacted_items_sample = random.sample(non_interacted_items, sample_size)
    non_interacted_items_list = list(non_interacted_items)
    non_interacted_items_sample = random.sample(non_interacted_items_list, sample_size)

    return set(non_interacted_items_sample)

#Top-N accuracy metrics
EVAL_RANDOM_SAMPLE_NON_INTERACTED_ITEMS = 100

class ModelEvaluator:

    # Function to verify whether a particular item_id was present in the set of top N recommended items
    def _verify_hit_top_n(self, item_id, recommended_items, topn):
            try:
                #Stores index of item id if it is present in the recommended_items
                index = next(i for i, c in enumerate(recommended_items) if c == item_id)
            except:
                #If item id is not found in the recommended list
                index = -1
            #checking whether index is present in the topN items or not
            hit = int(index in range(0, topn))
            return hit, index

    # Function to evaluate the performance of model for each user
    def evaluate_model_for_user(self, model, person_id):

        # Getting the items in test set
        interacted_values_testset = test_indexed_df.loc[person_id]

        #Obtaining unique interacted items by the user
        if type(interacted_values_testset['ISBN']) == pd.Series:
            person_interacted_items_testset = set(interacted_values_testset['ISBN'])
        else:
            person_interacted_items_testset = set([(interacted_values_testset['ISBN'])])

        interacted_items_count_testset = len(person_interacted_items_testset)

        # Getting a ranked recommendation list from the model for a given user
        person_recs_df = model.recommend_items(person_id, items_to_ignore=get_items_interacted(person_id, train_indexed_df),topn=10000000000)

        hits_at_5_count = 0
        hits_at_10_count = 0

        # For each item the user has interacted in test set
        for item_id in person_interacted_items_testset:

            # Getting a random sample of 100 items the user has not interacted with
            non_interacted_items_sample = get_not_interacted_items_sample(person_id, sample_size=100, seed=42)


            # Combining the current interacted item with the 100 random items
            items_to_filter_recs = non_interacted_items_sample.union(set([item_id]))

            # Filtering only recommendations that are either the interacted item or from a random sample of 100 non-interacted items
            valid_recs_df = person_recs_df[person_recs_df['ISBN'].isin(items_to_filter_recs)]
            valid_recs = valid_recs_df['ISBN'].values

            # Verifying if the current interacted item is among the Top-N recommended items
            hit_at_5, index_at_5 = self._verify_hit_top_n(item_id, valid_recs, 5)
            #Counting hit at 5
            hits_at_5_count += hit_at_5
            hit_at_10, index_at_10 = self._verify_hit_top_n(item_id, valid_recs, 10)
            #Counting hit at 10
            hits_at_10_count += hit_at_10

        # Recall is the rate of the interacted items that are ranked among the Top-N recommended items
        recall_at_5 = hits_at_5_count / float(interacted_items_count_testset)
        recall_at_10 = hits_at_10_count / float(interacted_items_count_testset)

        #Creating a dictionary
        person_metrics = {'hits@5_count':hits_at_5_count,
                          'hits@10_count':hits_at_10_count,
                          'interacted_count': interacted_items_count_testset,
                          'recall@5': recall_at_5,
                          'recall@10': recall_at_10}
        return person_metrics


    # Function to evaluate the performance of model at overall level
    def evaluate_model(self, model):

        people_metrics = []

        #Calculating metrics for all users in the test set
        for idx, person_id in enumerate(list(test_indexed_df.index.unique().values)):
            #Returns dictionary containing person_metrics for each user
            person_metrics = self.evaluate_model_for_user(model, person_id)
            #Adds user_id to the dictionary
            person_metrics['_person_id'] = person_id
            #Appends each dictionary to the list
            people_metrics.append(person_metrics)

        print('%d users processed' % idx)
        #Creates dataframe containing value of metrics for all the users using the list of dictionaries
        detailed_results_df = pd.DataFrame(people_metrics).sort_values('interacted_count', ascending=False)

        #Calculating global recall@5 and global recall@10
        global_recall_at_5 = detailed_results_df['hits@5_count'].sum() / float(detailed_results_df['interacted_count'].sum())
        global_recall_at_10 = detailed_results_df['hits@10_count'].sum() / float(detailed_results_df['interacted_count'].sum())

        #Creates dictionary containing global metrics
        global_metrics = {'modelName': model.get_model_name(),
                          'recall@5': global_recall_at_5,
                          'recall@10': global_recall_at_10}
        return global_metrics, detailed_results_df

model_evaluator = ModelEvaluator()

print('Evaluating Collaborative Filtering (SVD Matrix Factorization) model...')
cf_global_metrics, cf_detailed_results_df = model_evaluator.evaluate_model(cf_recommender_model)

print('\nGlobal metrics:\n%s' % cf_global_metrics)
cf_detailed_results_df.head(10)

import pickle

# Define the filename to save the model
model_filename = "final_model.pkl"

# Save the model to a file using pickle
with open(model_filename, 'wb') as file:
    pickle.dump(cf_recommender_model, file)

print(f"Model saved as {model_filename}")