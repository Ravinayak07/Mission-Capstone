# -*- coding: utf-8 -*-
"""parkinsons_Model_Training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KXrTPSFx-VrX_cvFMLiyd-vfx0_kjpUx

# Parkinsons Disease detection Using Machine Learning Algorithms
"""

# Commented out IPython magic to ensure Python compatibility.
# Built-in packages
import json
import warnings
import re
warnings.filterwarnings("ignore")

# Third party packages
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly
import plotly.graph_objects as go
import plotly.offline as pyo
from plotly.offline import iplot

from sklearn.preprocessing import StandardScaler
from sklearn import model_selection
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve

from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB

from mlxtend.classifier import StackingCVClassifier
from xgboost import XGBClassifier


sns.set(context= "notebook", color_codes=True)
plt.style.use('bmh')

pyo.init_notebook_mode()

# %matplotlib inline
pd.set_option('display.max_columns', None)

"""**name**$~~~~~~~~~~~~~~~~~~~~~~~~~$- ASCII subject name and recording number
____
**MDVP:Fo(Hz)**$~~~~~~~~~~~~~$- Average vocal fundamental frequency
____
**MDVP:Fhi(Hz)**$~~~~~~~~~~~~$- Maximum vocal fundamental frequency
____
**MDVP:Flo(Hz)**$~~~~~~~~~~~~$- Minimum vocal fundamental frequency
____
**MDVP:Jitter(%), <br>
MDVP:Jitter(Abs), <br>
MDVP:RAP, <br>
MDVP:PPQ, <br>
Jitter:DDP**$~~~~~~~~~~~~~~~~~~$- Several measures of variation in fundamental frequency
____
**MDVP:Shimmer, <br>
MDVP:Shimmer(dB), <br>
Shimmer:APQ3, <br>
Shimmer:APQ5, <br>
MDVP:APQ, <br>
Shimmer:DDA**$~~~~~~~~~~~~~$- Several measures of variation in amplitude
____
**NHR,HNR**$~~~~~~~~~~~~~~~~~~~$- Two measures of ratio of noise to tonal components in the voice
____
**status**$~~~~~~~~~~~~~~~~~~~~~~~~$- Health status of the subject (one) - Parkinson's, (zero) - healthy
____
**RPDE,D2**$~~~~~~~~~~~~~~~~~~~~$- Two nonlinear dynamical complexity measures
____
**DFA**$~~~~~~~~~~~~~~~~~~~~~~~~~~~$- Signal fractal scaling exponent
____
**spread1, <br>
spread2, <br>
PPE**$~~~~~~~~~~~~~~~~~~~~~~~~~~~$- Three nonlinear measures of fundamental frequency variation
____
"""

# Read the CSV file and store it in a DataFrame
df = pd.read_csv("parkinsons.data")

# Replace spaces, parentheses, colons, and percentage signs in column names with underscores
# Convert column names to lowercase
# This is done for consistency and to ensure compatibility with Python syntax
df.columns = [
    i.replace(" ", "_")  # Replace spaces with underscores
    .replace("(%)", "_perc")  # Replace percentage signs with _perc
    .replace("(dB)", "_db")  # Replace dB with _db
    .replace(":", "_")  # Replace colons with underscores
    .lower()  # Convert column names to lowercase
    for i in df.columns  # Iterate over each column name
]

# Remove parentheses and their content from column names using regular expressions
# This is done to make column names more concise and readable
df.columns = [re.sub(r"\((.+)\)", "", i) for i in df.columns]
df

print(f"The shape of the DatFrame is: {df.shape}, which means there are {df.shape[0]} rows and {df.shape[1]} columns.")

df.info()

# Checking if any of the columns have null values
print(df.isnull().sum())

# Generating summary statistics for numerical columns in the DataFrame
df_summary = df.describe()

# Displaying the summary statistics DataFrame
# This provides an overview of the central tendency, dispersion, and shape of the numerical data
df_summary

df["status"].value_counts()

# Genrating a count plot to visualize the distribution of the "status" column in the DataFrame
# The x-axis represents the values of the "status" column
# The data parameter specifies the DataFrame from which the data is retrieved
sns.countplot(x="status", data=df, palette="Set2")

df.groupby("status").mean()

# Checking for missing values (NaN) in the DataFrame
# The isna() method returns a DataFrame of the same shape as df, where each element is True if the corresponding element in df is NaN, and False otherwise
# The sum() method is then applied to count the number of missing values in each column
df.isna().sum()

# Create a figure with a specified size (20x20 inches) for the heatmap plot
plt.figure(figsize=(20, 20))

# Generate a heatmap to visualize the correlation matrix of the DataFrame
# The corr() method computes the pairwise correlation of columns, excluding NaN values
# The heatmap() function from Seaborn is used to create a heatmap plot
sns.heatmap(df.corr(), fmt='.2f', annot=True);

# Removing the columns "name" and "status" from the DataFrame and store the result in variable x
# The drop() function is used to remove columns along the specified axis (axis=1 for columns)
# The columns to be removed are specified in the list ["name", "status"]
x = df.drop(columns=["name", "status"], axis=1)

# Storing the "status" column as the target variable y
# This column represents the labels or target values that the model aims to predict
y = df["status"]

# This displays the DataFrame x, which contains all columns except "name" and "status"
print(x)

print(y)

# Defining a function that computes value counts for a specified column, segmented by the "status" column
def groupby_get_cc_count(tdf, col):
    # Grouping the DataFrame by the specified column and "status", count occurrences, and reset index
    tdf = tdf.groupby([col, "status"])["status"].count().reset_index(level=0)
    tdf.columns = [col, "count"]
    tdf = tdf.reset_index()
    return tdf

df[["mdvp_fo", "mdvp_jitter", "status"]]

def draw_axvlines(plt, col):
    mean = df_summary.loc["mean", col]
    q1 = df_summary.loc["25%", col]
    q2 = df_summary.loc["50%", col]
    q3 = df_summary.loc["75%", col]
    plt.axvline(mean, color = "g");              # Plotting a line to mark the mean
    plt.axvline(q1, color = "b");                # Plotting a line to mark Q1
    plt.axvline(q2, color = "navy");             # Plotting a line to mark Q2
    plt.axvline(q3, color = "purple");           # Plotting a line to mark Q3
    plt.legend({"Mean": mean, "25%" : q1, "50%" : q2, "75%" : q3});

fig, axes = plt.subplots(3, 2, figsize = (20,15));
fig.suptitle('Distribution charts for Age, Experience and income.');


# Create boxplot to show distribution of Age
sns.boxplot(df["mdvp_fo"], ax = axes[0][0], color = "mediumslateblue");
axes[0][0].set(xlabel = 'Distribution of Age');

pp = sns.distplot(df["mdvp_fo"], ax = axes[0][1], bins = 10, color = "mediumslateblue");
axes[0][1].set(xlabel = 'Distribution of Age');
draw_axvlines(pp, "mdvp_fo");


# Create boxplot to show distribution of creatinine_phosphokinase
sns.boxplot(df["mdvp_fhi"], ax = axes[1][0], color = "mediumslateblue");
axes[1][0].set(xlabel = 'Distribution of creatinine_phosphokinase');

pp = sns.distplot(df["mdvp_fhi"], ax = axes[1][1], bins = 10, color = "mediumslateblue");
axes[1][1].set(xlabel = 'Distribution of creatinine_phosphokinase');
draw_axvlines(pp, "mdvp_fhi")


# Create boxplot to show distribution of platelets
sns.boxplot(df["mdvp_flo"], ax = axes[2][0], color = "mediumslateblue");
axes[2][0].set(xlabel = 'Distribution of platelets');

pp = sns.distplot(df["mdvp_flo"], ax = axes[2][1], color = "mediumslateblue");
axes[2][1].set(xlabel = 'Distribution of platelets');
draw_axvlines(pp, "mdvp_flo")

def draw_axvlines(plt, col):
    mean = df_summary.loc["mean", col]
    q1 = df_summary.loc["25%", col]
    q2 = df_summary.loc["50%", col]
    q3 = df_summary.loc["75%", col]
    plt.axvline(mean, color = "g");              # Plotting a line to mark the mean
    plt.axvline(q1, color = "b");                # Plotting a line to mark Q1
    plt.axvline(q2, color = "navy");             # Plotting a line to mark Q2
    plt.axvline(q3, color = "purple");           # Plotting a line to mark Q3
    plt.legend({"Mean": mean, "25%" : q1, "50%" : q2, "75%" : q3});

fig, axes = plt.subplots(1, 1, figsize = (10,6));
fig.suptitle('Distribution chart for Age');


# Create a violin plot to show the distribution of Age
sns.boxplot(data=df["mdvp_fo"], color="salmon")
plt.show()

# Create subplots with a single plot, specifying the figure size
fig, axes = plt.subplots(1, 1, figsize=(10, 6))
fig.suptitle('Kernel Density Estimation Plot for Age Distribution')

# The kdeplot() function from Seaborn is used to generate the plot
pp = sns.kdeplot(data=df["mdvp_fo"], color="salmon", fill=True)

# Calling the draw_axvlines function to draw vertical lines indicating statistical measures
draw_axvlines(pp, "mdvp_fo")

fig, axes = plt.subplots(1, 1, figsize=(10, 6))
fig.suptitle('Distribution of creatinine_phosphokinase')

# Create boxplot to show distribution of creatinine_phosphokinase
sns.boxplot(df["mdvp_fhi"], color = "blue");


# # Create boxplot to show distribution of platelets
# sns.boxplot(df["mdvp_flo"], ax = axes[2][0], color = "mediumslateblue");
# axes[2][0].set(xlabel = 'Distribution of platelets');

# pp = sns.distplot(df["mdvp_flo"], ax = axes[2][1], color = "mediumslateblue");
# axes[2][1].set(xlabel = 'Distribution of platelets');
# draw_axvlines(pp, "mdvp_flo")

# Creating a copy of the DataFrame "df" and remove the column "name"
df_train = df.copy().drop(columns=["name"])

# Extracting the column names from the modified DataFrame and store them in a list
col_names = df_train.columns.tolist()

# Specifying the target column as "status"
target_col = ["status"]

# Removing the target column from the list of column names
col_names.remove(target_col[0])

# Rearranging the columns in the DataFrame by placing the target column at the end
df_train = df_train[col_names + target_col]

# Set the figure size for the heatmap plot
plt.figure(figsize=(15, 10))

# Compute the correlation matrix for the DataFrame "df_train"
corr = df_train.corr()

# Generate a heatmap to visualize the correlation matrix
# The heatmap() function from Seaborn is used to create the heatmap plot
sns.heatmap(corr, annot=True, fmt='.2g', cmap='coolwarm')

"""## Standardization: transforms numerical features to have a mean of 0 and a standard deviation of"""

# Creating a StandardScaler object for standardization
std = StandardScaler()

# The fit_transform() method fits the scaler to the data and transforms it simultaneously
scaled = std.fit_transform(df_train[col_names])

# Converting the scaled array into a DataFrame with column names preserved
scaled = pd.DataFrame(scaled, columns=col_names)

# Concatenating the scaled DataFrame with the target column "status" to create the final training DataFrame
df_train = pd.concat([scaled, df_train[target_col]], axis=1)

# Displaying the first few rows of the modified DataFrame
df_train.head()

X = df_train[col_names]      # Contains the independent columns
y = df_train[target_col]     # Our target column

train_X, test_X, train_y, test_y = train_test_split(X, y, test_size = 0.3, random_state = 323)
train_y = train_y["status"]
test_y = test_y["status"]

conf_matrix_all = {}

def parkinsons_disease_prediction(name, algo, training_x, testing_x, training_y, testing_y, plot) :
    algo.fit(training_x,training_y)                           # Fit the training data set to the algorithm passed.
    predictions = algo.predict(testing_x)                     # Get all predictions
    probabilities = algo.predict_proba(testing_x)             # Get probablities of predictions

    conf_matrix = confusion_matrix(testing_y, predictions)    # Get confusion matrix using the predictions
    tn, fp, fn, tp = conf_matrix.ravel()

    conf_matrix_all[name] = conf_matrix                       # Save confusion matrix values to a dictionary

    print("Classification report:")                           # Print the classification report
    print(classification_report(testing_y, predictions))

    model_roc_auc = roc_auc_score(testing_y, predictions)           # Get the Area under the curve number
    fpr,tpr,thresholds = roc_curve(testing_y, probabilities[:,1])   # Get False postive rate and true positive rate

"""### K-nearest Neighbors"""

knn = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='manhattan', metric_params=None, n_neighbors = 2, weights='distance')

parkinsons_disease_prediction("K-Nearest Neighbours", knn, train_X, test_X, train_y, test_y, plot=True)

"""### Logistic Regression"""

lr  = LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, penalty="l1", solver='liblinear')

parkinsons_disease_prediction("Logistic Regression", lr, train_X, test_X, train_y, test_y, plot = True)

"""### Naïve Bayes"""

gnb = GaussianNB(priors=None, var_smoothing=1e-09)

parkinsons_disease_prediction("Gaussian Naïve Bayes", gnb, train_X, test_X, train_y, test_y, plot=True)

"""### Support Vector Classifier"""

svc = SVC(C=1.0, kernel='linear', degree= 2, gamma=1.0, random_state=None,
          coef0=0.0, shrinking=True, probability=True,tol=0.001,
          cache_size=200, class_weight=None, verbose=False,max_iter= -1)

parkinsons_disease_prediction("Support Vector Classifier", svc, train_X, test_X, train_y, test_y, plot=True)

"""## Ensemble technique employed:
Stacking Classifier (StackingCVClassifier) combining Logistic Regression, K-nearest Neighbors, and Support Vector Classifier predictions, evaluated using cross-validation and compared against individual classifiers.
"""

sclf = StackingCVClassifier(classifiers=[lr, knn, svc],
                            meta_classifier=LogisticRegression(),
                            random_state=42)

for clf, label in zip([lr, knn, svc, sclf], ["Logistic Regression" , 'KNN', 'Support Vector', 'StackingClassifier']):

    scores = model_selection.cross_val_score(clf, X, y, cv=3, scoring='accuracy')
    print("Accuracy: %0.2f (+/- %0.2f) [%s]" % (scores.mean(), scores.std(), label))

print("\n")
parkinsons_disease_prediction("Stacking Classifier", sclf, train_X, test_X, train_y, test_y, plot=True)

import joblib

# Train the stacking classifier
sclf.fit(train_X, train_y)

# Save the trained model to a file
joblib.dump(sclf, 'ensembled_model.pkl')

print("Ensembled model saved successfully.")