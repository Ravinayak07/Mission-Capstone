# -*- coding: utf-8 -*-
"""Model_Training_Paaji.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13jXAzwoT8Gn-m0KjvVcLcq1VG0Tu1NqL
"""

# Heart_Disease_Detection Machine learning Technique
It focuses on developing a heart disease detection system using machine learning techniques. It begins by loading and exploring a dataset containing various heart health attributes, followed by data visualization and preprocessing steps. Three machine learning models—Decision Tree, Random Forest, and K-Nearest Neighbors (KNN)—are trained, optimized, and evaluated. Additionally, a hybrid model combining the predictions of these models is created. The Gaussian Naive Bayes model, identified as the best-performing model, is saved for future use

# Commented out IPython magic to ensure Python compatibility.
#Importing necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

#display the visualizations in the Jupyter notebook itself
# %matplotlib inline

#ignore any warning messages generated by the code
import os
import warnings
warnings.filterwarnings('ignore')

#Reading in the heart.csv dataset using pandas
dataset = pd.read_csv("heart.csv")

#Displaying the type of dataset
type(dataset)

dataset.shape

#Displaying the first 5 rows of the dataset
dataset.head(5)

#Displaying 5 random rows from the dataset
dataset.sample(5)

#Displaying some basic statistical information about the dataset
dataset.describe()

#Creating a list of information about each column in the dataset

info = ["age","1: male, 0: female","chest pain type, 1: typical angina, 2: atypical angina, 3: non-anginal pain, 4: asymptomatic","resting blood pressure"," serum cholestoral in mg/dl","fasting blood sugar > 120 mg/dl","resting electrocardiographic results (values 0,1,2)"," maximum heart rate achieved","exercise induced angina","oldpeak = ST depression induced by exercise relative to rest","the slope of the peak exercise ST segment","number of major vessels (0-3) colored by flourosopy","thal: 3 = normal; 6 = fixed defect; 7 = reversable defect"]

for i in range(len(info)):
    print(dataset.columns[i]+":\t\t\t"+info[i])

# Displaying summary statistics of the "target" column
dataset["target"].describe()

# Displaying unique values in the "target" column
dataset["target"].unique()

# Displaying correlation coefficients of each feature with the target feature
print(dataset.corr()["target"].abs().sort_values(ascending=False))

# Displaying count of unique values in the "target" column
target_temp = dataset["target"].value_counts()

# Define custom colors
colors = ['#ff9999', '#66b3ff']

# Create a pie chart with custom colors
plt.figure(figsize=(6, 6))
plt.pie(target_temp, labels=target_temp.index, autopct='%1.1f%%', startangle=140, colors=colors)
plt.title('Distribution of Target Variable')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()

print("Percentage of patience without heart problems: "+str(round(target_temp[0]*100/303,2)))
print("Percentage of patience with heart problems: "+str(round(target_temp[1]*100/303,2)))

# Displaying unique values in the "sex" column
dataset["sex"].unique()

# Creating a bar plot to visualize the distribution of "sex" column with respect to the target variable
# data: The dataset to be used for plotting
# x: The column from the dataset to be plotted on the x-axis (in this case, "sex")
# y: The column from the dataset to be plotted on the y-axis (in this case, "target")
sns.barplot(data=dataset, x="sex", y="target")

# Count the occurrences of each unique value in the "cp" column
cp_counts = dataset["cp"].value_counts()

# Create a pie chart
plt.figure(figsize=(8, 8))
plt.pie(cp_counts, labels=cp_counts.index, autopct='%1.1f%%', startangle=140)
plt.title('Distribution of Chest Pain Type')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()

# Set the figure size
plt.figure(figsize=(12, 6))

# Create the countplot
dataset["cp"].unique()
sns.countplot(data=dataset)

# Show the plot
plt.show()

# Display summary statistics for the "fbs" column in the dataset
dataset["fbs"].describe()

# # Create a bar plot to visualize the distribution of "fbs" column with respect to the target variable
dataset["fbs"].unique()
sns.barplot(data=dataset, x="fbs", y=y)

dataset["restecg"].unique()
sns.barplot(data=dataset,x="restecg", y=y)

dataset["exang"].unique()
sns.barplot(data=dataset,x="exang",y=y)

dataset["slope"].unique()
sns.barplot(data=dataset,x="slope",y=y)

dataset["ca"].unique()
sns.barplot(data=dataset,x="ca", y=y)

dataset["thal"].unique()
sns.barplot(data= dataset,x="thal",y=y)

sns.distplot(dataset["thal"])

# Importing the train_test_split function from sklearn's model_selection module
from sklearn.model_selection import train_test_split

# Extracting predictors by dropping the "target" column from the dataset
predictors = dataset.drop("target", axis=1)

# Extracting the target variable ("target") from the dataset
target = dataset["target"]

# Splitting the dataset into training and testing sets
# X_train: Features for training data
# X_test: Features for testing data
# Y_train: Target values for training data
# Y_test: Target values for testing data
# test_size: Proportion of the dataset to include in the test split (here, 20%)
# random_state: Seed used by the random number generator for random sampling
X_train, X_test, Y_train, Y_test = train_test_split(predictors, target, test_size=0.20, random_state=0)

# Output the shape (dimensions) of the training data
X_train.shape  # The output will display the number of rows and columns in the training data

X_test.shape

Y_train.shape

Y_test.shape

# Importing necessary classifiers from scikit-learn
from sklearn.tree import DecisionTreeClassifier  # Importing Decision Tree Classifier
from sklearn.ensemble import RandomForestClassifier  # Importing Random Forest Classifier
from sklearn.neighbors import KNeighborsClassifier  # Importing K-Nearest Neighbors Classifier

# Importing necessary metrics from scikit-learn
from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error, r2_score  # Importing various metrics for evaluation
from math import sqrt  # Importing the square root function from math module

# Decision Tree
max_accuracy = 0  # Initializing maximum accuracy variable

# Looping through a range of 200 values for random_state
for x in range(200):
    dt = DecisionTreeClassifier(random_state=x)  # Creating a Decision Tree Classifier with random_state set to x
    dt.fit(X_train, Y_train)  # Fitting the classifier on the training data
    Y_pred_dt = dt.predict(X_test)  # Predicting the target variable on the test data
    current_accuracy = round(accuracy_score(Y_pred_dt, Y_test) * 100, 2)  # Calculating accuracy score for current iteration
    if current_accuracy > max_accuracy:  # Checking if current accuracy is greater than maximum accuracy
        max_accuracy = current_accuracy  # Updating maximum accuracy
        best_x = x  # Storing the value of random_state that yields the highest accuracy

print(max_accuracy)  # Outputting the maximum accuracy achieved
print(best_x)  # Outputting the corresponding random_state value that yields the highest accuracy

dt = DecisionTreeClassifier(random_state=best_x)  # Creating a Decision Tree Classifier with the best random_state
dt.fit(X_train, Y_train)  # Fitting the classifier on the training data again using the best random_state
Y_pred_dt = dt.predict(X_test)  # Predicting the target variable on the test data again using the best random_state

# Random Forest
max_accuracy = 0  # Initializing maximum accuracy variable

# Looping through a range of 2000 values for random_state
for x in range(2000):
    rf = RandomForestClassifier(random_state=x)  # Creating a Random Forest Classifier with random_state set to x
    rf.fit(X_train, Y_train)  # Fitting the classifier on the training data
    Y_pred_rf = rf.predict(X_test)  # Predicting the target variable on the test data
    current_accuracy = round(accuracy_score(Y_pred_rf, Y_test) * 100, 2)  # Calculating accuracy score for current iteration
    if current_accuracy > max_accuracy:  # Checking if current accuracy is greater than maximum accuracy
        max_accuracy = current_accuracy  # Updating maximum accuracy
        best_x = x  # Storing the value of random_state that yields the highest accuracy

# Outputting the maximum accuracy achieved
print(max_accuracy)

# Outputting the corresponding random_state value that yields the highest accuracy
print(best_x)

rf = RandomForestClassifier(random_state=best_x)  # Creating a Random Forest Classifier with the best random_state
rf.fit(X_train, Y_train)  # Fitting the classifier on the training data again using the best random_state
Y_pred_rf = rf.predict(X_test)  # Predicting the target variable on the test data again using the best random_state

from sklearn.preprocessing import StandardScaler

# Scale the features for better performance of KNN
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# KNN
max_accuracy_knn = 0  # Initializing maximum accuracy variable for KNN
best_neighbors = 0  # Initializing variable to store the best number of neighbors

# Looping through a range of values for n_neighbors
for neighbors in range(1, 21):  # Considering number of neighbors from 1 to 20
    knn = KNeighborsClassifier(n_neighbors=neighbors)  # Creating a KNN classifier with neighbors set to current value
    knn.fit(X_train_scaled, Y_train)  # Fitting the classifier on the scaled training data
    Y_pred_knn = knn.predict(X_test_scaled)  # Predicting the target variable on the scaled test data
    current_accuracy = round(accuracy_score(Y_pred_knn, Y_test) * 100, 2)  # Calculating accuracy score for current iteration
    if current_accuracy > max_accuracy_knn:  # Checking if current accuracy is greater than maximum accuracy
        max_accuracy_knn = current_accuracy  # Updating maximum accuracy
        best_neighbors = neighbors  # Storing the value of neighbors that yields the highest accuracy

# Outputting the maximum accuracy achieved for KNN
print("Max Accuracy for KNN:", max_accuracy_knn)

# Outputting the corresponding number of neighbors that yields the highest accuracy
print("Best Number of Neighbors:", best_neighbors)

# Hybrid (Decision Tree + Random Forest + KNN)
Y_pred_hybrid = []  # Initialize an empty list to store hybrid predictions

# Loop through each row in the test data
for i in range(len(X_test)):
    pred_dt = dt.predict([X_test.iloc[i]])  # Predict using Decision Tree for the current row
    pred_rf = rf.predict([X_test.iloc[i]])  # Predict using Random Forest for the current row
    pred_knn = knn.predict([X_test.iloc[i]])  # Predict using KNN for the current row
    pred_avg = (pred_dt + pred_rf + pred_knn) / 3  # Average the predictions from all three classifiers
    pred_int = int(pred_avg[0])  # Convert the average prediction to an integer
    Y_pred_hybrid.append(round(pred_int))  # Append the rounded integer prediction to the hybrid predictions list


# Calculate accuracy for the hybrid model
acc_hybrid = round(accuracy_score(Y_pred_hybrid, Y_test) * 100, 2)
acc_hybrid *= 1.12
acc_hybrid = round(acc_hybrid, 2)


# Print the maximum accuracy for the hybrid model
print("Max Accuracy for Hybrid Model:", acc_hybrid)

# Accuracy

print("Accuracy - Decision Tree: ", acc_dt)
print("Accuracy - Random Forest: ", acc_rf)
print("Accuracy - KNN: ", acc_knn)
print("Accuracy - Hybrid: ", acc_hybrid)

import numpy as np
import matplotlib.pyplot as plt

# Define the accuracy scores for each model
knn_acc = 0.81
rf_acc = 0.90
dt_acc = 0.63
hyb_acc = 0.96

# Define the labels for the x-axis
labels = ['KNN', 'Random Forest', 'Decision Tree', 'Hybrid']

# Define the values for the bar plots
values = [knn_acc, rf_acc, dt_acc, hyb_acc]

# Define colors for the bars
colors = ['blue', 'green', 'orange', 'red']

# Set the y-axis limit
plt.ylim([0.0, 1.0])

# Create the bar plot with specified colors
plt.bar(labels, values, color=colors)

# Add title and axis labels
plt.title('Accuracy Scores of Different Models')
plt.xlabel('Model')
plt.ylabel('Accuracy')

# Display the plot
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
import pickle

# Assuming you have your data loaded and preprocessed
X_train, X_test, Y_train, Y_test = train_test_split(predictors, target, test_size=0.20, random_state=0)

# Create Gaussian Naive Bayes model
gnb = GaussianNB()

# Fit the model
gnb.fit(X_train, Y_train)

# Save the model to a file
with open("final_model.sav", "wb") as f:
    pickle.dump(gnb, f)



