Before training anything, we first organized the dataset into three groups — female speech, male speech, and background noise (called Noizeus). We used Librosa to load all audio files and double-checked that their sampling rates were the same, which avoids issues later. Only .wav and .mp3 files were kept since those work best with the libraries we used. After that, we checked how evenly the files were distributed across categories. A quick visualization confirmed that things looked pretty balanced overall [9].

Next, each audio file was turned into a mel-spectrogram. This step changed raw audio into an image that shows how sound frequencies change over time — something convolutional neural networks are really good at understanding. All of these spectrograms were saved as .png images and resized so they’d be the same size. Doing this helped the model take in consistent input and made the training process smoother from a technical standpoint [10].

For the model architecture, we used a convolutional neural network (CNN). It had a few convolutional layers, pooling layers, and then some dense layers at the end for classification. To make the training more flexible, we used Keras’s ImageDataGenerator to add pitch shifts and noise on the fly. This made the model see different versions of the same data every time. We used categorical cross-entropy for the loss function and the Adam optimizer to help it learn faster. Early stopping was added so training would stop once the model stopped improving [11].

While the model trained, we watched both training and validation accuracy to make sure it wasn’t overfitting. Things improved steadily without big swings in performance, which was a good sign. Dropout layers helped avoid overfitting, and learning rate scheduling gave the model time to settle into good values. Also, since we augmented data in real-time, the model didn’t need too much memory. This setup worked out well, especially for noisy and unpredictable audio [12]. 
